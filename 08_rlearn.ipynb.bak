{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://hilpisch.com/taim_logo.png' width=\"350px\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Yves J Hilpisch | The AI Machine\n",
    "\n",
    "http://aimachine.io | http://twitter.com/dyjh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/tpq-classes/rl_for_finance.git\n",
    "import sys\n",
    "sys.path.append('rl_for_finance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque\n",
    "plt.style.use('seaborn-v0_8')\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://gym.openai.com/envs/LunarLander-v2/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space  # type of action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n  # number of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do nothing, fire left orientation engine, fire main engine, fire right orientation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()  # sample action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()  # sample action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 3, 2, 1, 1, 0, 2, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
       "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space  # type of observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5 ,  2.5 , 10.  , 10.  ,  6.28, 10.  ,  1.  ,  1.  ],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high.astype(np.float16) # upper bounds for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.5 ,  -2.5 , -10.  , -10.  ,  -6.28, -10.  ,  -0.  ,  -0.  ],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low.astype(np.float16)  # lower bounds for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0049,  1.4047,  0.4918, -0.2753, -0.0056, -0.1114,  0.    ,\n",
       "         0.    ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.reset()\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualizes the effect of a number of random actions taken. See https://gist.github.com/thomelane/79e97630ba46c45985a946cae4805885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()  # random action\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0096,  1.3979,  0.4793, -0.3014, -0.0087, -0.0625,  0.    ,\n",
       "         0.    ], dtype=float32),\n",
       " 0.07787057798020783,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = env.step(a)  # taking action, capturing new observations\n",
    "r  # (observation, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** FAILED ***\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render()) # initialize bitmap embedding\n",
    "for e in range(201):\n",
    "    img.set_data(env.render()) # updating the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    a = env.action_space.sample()  # random action choice\n",
    "    # a = 3  # costant action choice\n",
    "    obs, rew, done, trunc, info = env.step(a)  # taking action\n",
    "    if done and (e + 1) < 200:\n",
    "        print('*** FAILED ***')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0  # initial epsilon\n",
    "        self.epsilon_min = 0.01  # minimal epsilon\n",
    "        self.epsilon_decay = 0.995  # epsilon decay\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.batch_size = 128  # batch size for replay\n",
    "        self.max_treward = -1e6\n",
    "        self.averages = list()\n",
    "        self.memory = deque(maxlen=2000)  # fixed memory\n",
    "        self.osn = env.observation_space.shape[0]\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1024, input_dim=self.osn,\n",
    "                        activation='relu'))\n",
    "        # model.add(Dense(256, activation='relu'))\n",
    "        # multiple labels, discrete actions\n",
    "        # estimation problem (activation is linear)\n",
    "        model.add(Dense(env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "        return model\n",
    "        \n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        action = self.model.predict(state)\n",
    "        return np.argmax(action)  # choose action with highest value\n",
    "    \n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            reward -= np.mean(state[:1] ** 2)  # distance to origin\n",
    "            if not done:\n",
    "                reward += self.gamma * np.amax(\n",
    "                    self.model.predict(next_state))\n",
    "            target = self.model.predict(state)\n",
    "            target[0, action] = reward\n",
    "            self.model.fit(state, target, epochs=1,\n",
    "                           verbose=False)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def learn(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            state = np.reshape(state, [1, self.osn])\n",
    "            treward = 0\n",
    "            for _ in range(5000):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, info = env.step(action)\n",
    "                next_state = np.reshape(next_state,\n",
    "                                        [1, self.osn])\n",
    "                self.memory.append([state, action, reward,\n",
    "                                     next_state, done])\n",
    "                state = next_state\n",
    "                treward += float(reward)\n",
    "                if done:\n",
    "                    trewards.append(treward)\n",
    "                    av = sum(trewards[-25:]) / 25\n",
    "                    self.averages.append(av)\n",
    "                    self.max_treward = max(self.max_treward, treward)\n",
    "                    templ = 'episode: {:4d}/{} | treward: {:7.1f} | '\n",
    "                    templ += 'av: {:7.1f} | max: {:7.1f}'\n",
    "                    print(templ.format(e, episodes, treward, av,\n",
    "                                       self.max_treward), end='\\r')\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "            if treward > 200:\n",
    "                break\n",
    "        print()\n",
    "                \n",
    "    def test(self, episodes):\n",
    "        trewards = []\n",
    "        for e in range(1, episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            treward = 0\n",
    "            for _ in range(1001):\n",
    "                state = np.reshape(state, [1, self.osn])\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, trunc, info = env.step(action)\n",
    "                state = next_state\n",
    "                treward += float(reward)\n",
    "                if done:\n",
    "                    trewards.append(treward)\n",
    "                    print('episode: {:4d}/{} | treward: {:7.1f}'\n",
    "                          .format(e, episodes, treward), end='\\r')\n",
    "                    break\n",
    "        return trewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.api._v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seeds(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQLAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mDQLAgent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)  \u001b[38;5;66;03m# fixed memory\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mosn \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 22\u001b[0m, in \u001b[0;36mDQLAgent._build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model.add(Dense(256, activation='relu'))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# multiple labels, discrete actions\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# estimation problem (activation is linear)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/anaconda/envs/tensorflow-2.10/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:58\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m---> 58\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m/anaconda/envs/tensorflow-2.10/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/tensorflow-2.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.api._v2'"
     ]
    }
   ],
   "source": [
    "set_seeds(100)\n",
    "agent = DQLAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50 # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time agent.learn(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(agent.averages))\n",
    "y = np.polyval(np.polyfit(x, agent.averages, deg=3), x)\n",
    "plt.plot(agent.averages, label='moving average')\n",
    "plt.plot(x, y, 'r--', label='regression')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('total reward')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trewards = agent.test(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(trewards) / len(trewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list()\n",
    "for seed in range(10000, 15001, 100):\n",
    "    env.action_space.seed(seed)\n",
    "    state, _ = env.reset()\n",
    "    for e in range(501):\n",
    "        state = np.reshape(state, [1, agent.osn])\n",
    "        a = np.argmax(agent.model.predict(state))  # learned action\n",
    "        state, reward, done, trunc, info = env.step(a)\n",
    "        if done and reward == 100:\n",
    "            print(f'*** FINISHED *** ({seed} | {e})'  + 20 * ' ', end='\\r')\n",
    "            seeds.append((seed, e))\n",
    "            break\n",
    "        elif done and reward == -100:\n",
    "            print(f'*** FAILED *** ({e})' + 20 * ' ', end='\\r')\n",
    "            break\n",
    "    if not done:\n",
    "        print(f'*** REACHED ITERATION MAX ({seed}) ***', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.seed(11500)\n",
    "state, _ = env.reset()\n",
    "img = plt.imshow(env.render()) # initialize bitmap embedding\n",
    "for e in range(501):\n",
    "    img.set_data(env.render()) # updating the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state = np.reshape(state, [1, agent.osn])\n",
    "    a = np.argmax(agent.model.predict(state))  # learned action\n",
    "    state, reward, done, trunc, info = env.step(a)\n",
    "    if done and reward == 100:\n",
    "        print(f'*** FINISHED ({e}) ***')\n",
    "        break\n",
    "    elif done and reward == -100:\n",
    "        print(f'*** FAILED *** ({e})')\n",
    "        break\n",
    "if not done:\n",
    "    print('*** REACHED ITERATION MAX ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://hilpisch.com/taim_logo.png' width=\"350px\" align=\"right\">\n",
    "\n",
    "<br><br><br><a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:ai@tpq.io\">ai@tpq.io</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}